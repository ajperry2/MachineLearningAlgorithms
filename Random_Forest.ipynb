{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T22:42:20.307107Z",
     "start_time": "2019-05-23T22:42:20.293899Z"
    }
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "### A random forest keeps a collection of decision trees. Based on the type of variable you are predicting you either find the mean of the tree's predictions or you find the most common prediction (in the case of a catagorical response variable).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T06:57:29.133293Z",
     "start_time": "2019-05-24T06:57:29.130337Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T20:20:52.269628Z",
     "start_time": "2019-05-24T20:20:52.179498Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self,X:np.ndarray,y:np.ndarray,num_trees=1,max_depth=3,num_splits=10):\n",
    "        '''\n",
    "        Random Forest:\n",
    "        Parameters:\n",
    "            X: An array of numerical factors\n",
    "            y: The responding variable, can be categorical or continuous\n",
    "            num_trees: The number of estimators which are voting to form a prediction\n",
    "            max_depth: The maximum number of nodes from root to leaves in each estimator\n",
    "            num_splits: The number of splits to be tested for each factor at each node\n",
    "        '''\n",
    "        self.trees = []\n",
    "        self.__is_continuous__ = self.__is_continuous__(y)\n",
    "\n",
    "        for tree in range(num_trees):\n",
    "            if self.__is_continuous__:\n",
    "                self.trees.append(self.__DecisionTreeRegressor__(X,y,max_depth,num_splits))\n",
    "            else:\n",
    "                self.trees.append(self.__DecisionTreeClassifier__(X,y,max_depth,num_splits))\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Predict:\n",
    "        Parameters:\n",
    "            X: An array of numerical factors\n",
    "            \n",
    "        Returns:\n",
    "            A series of predictions, the continuity of predictions depends on the labels the model was trained on\n",
    "        '''\n",
    "        total_predictions = []\n",
    "        for x in X:\n",
    "            predictions = []\n",
    "            for tree in self.trees:\n",
    "                predictions.append(tree.predict(x))\n",
    "\n",
    "            if self.__is_continuous__:\n",
    "                'For regression we find the mean of the predictions'\n",
    "\n",
    "                total_predictions.append(np.array(predictions).mean())\n",
    "            else:\n",
    "                'For classification we find the most common prediction'\n",
    "                counts = {}\n",
    "                for prediction in predictions:\n",
    "                    if prediction in counts:\n",
    "                        counts[prediction] += 1\n",
    "                    else:\n",
    "                        counts[prediction] = 1\n",
    "                most = 0\n",
    "                most_key=''\n",
    "                for key in counts.keys():\n",
    "                    if counts[key] > most:\n",
    "                        most = counts[key]\n",
    "                        most_key=key\n",
    "                total_predictions.append( most_key)    \n",
    "        return np.array(total_predictions)\n",
    "    class __DecisionTreeClassifier__():\n",
    "        def __init__(self,X:np.ndarray,y:np.ndarray,max_depth,num_splits):\n",
    "            \"Constructs a Decision Tree which uses Information Gain to choose the best splits\"\n",
    "            self.tree = self.Node_(X,y,self,num_splits,max_depth)\n",
    "        def predict(self,x):\n",
    "            \"Predicts using the established tree\"\n",
    "            node = self.tree\n",
    "            while node.prediction_value is None:\n",
    "                #decend tree\n",
    "                if x[node.split_characteristic] <= node.split: node = node.left\n",
    "                else: node = node.right\n",
    "            return node.prediction_value\n",
    "        class Node_():\n",
    "            def __init__(self, X,y,tree, num_splits, max_depth,current_depth=0):\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "\n",
    "                self.tree = tree\n",
    "                #calculate entropy\n",
    "                classes = np.unique(y)\n",
    "                if len(classes) == 1:\n",
    "                    self.prediction_value = classes[0]\n",
    "                    # early stop as we have reached a pur\n",
    "                    return\n",
    "                p_classes = []\n",
    "                for class_ in classes:\n",
    "                    p_class = np.sum(y==class_)/len(y)\n",
    "                    p_classes.append(p_class)\n",
    "\n",
    "                self.entropy = -np.sum(np.array([p_class*np.log2(p_class) for p_class in p_classes]))\n",
    "\n",
    "\n",
    "                info_gains = [] # used to find best split\n",
    "                total_splits = []\n",
    "\n",
    "                for column_index in range(X.shape[1]):\n",
    "                    curr_column = X[:,column_index]\n",
    "\n",
    "                    #pick 10 random potential split points\n",
    "\n",
    "                    random_splits = np.random.random_sample(num_splits,)*(curr_column.max()-curr_column.min())\\\n",
    "                                    + curr_column.min()\n",
    "                    total_splits = np.concatenate([total_splits, random_splits],axis=0)\n",
    "\n",
    "                    # decide on split using information gain\n",
    "                    for split in random_splits:\n",
    "                        \n",
    "                        y_lower = y[curr_column<=split]\n",
    "                        y_higher = y[curr_column>split]\n",
    "                        lower_p_classes = []\n",
    "                        higher_p_classes = []\n",
    "                        for class_ in classes:\n",
    "                            lower_p_class = np.sum(y_lower==class_)/len(y_lower)\n",
    "                            lower_p_classes.append(lower_p_class)    \n",
    "                            higher_p_class = np.sum(y_higher==class_)/len(y_higher)\n",
    "                            higher_p_classes.append(higher_p_class)    \n",
    "                        lower_entropy = -np.sum(np.array([p_class*np.log2(p_class) for p_class in lower_p_classes]))\n",
    "                        higher_entropy = -np.sum(np.array([p_class*np.log2(p_class) for p_class in higher_p_classes]))\n",
    "                        info_gain = self.entropy -  higher_entropy - lower_entropy\n",
    "                        info_gains.append(info_gain)\n",
    "                # split using best splitpoint\n",
    "                arg_max = np.argmax(np.array(info_gains))\n",
    "                self.split_characteristic = arg_max // len(random_splits)\n",
    "                final_split = total_splits[arg_max]\n",
    "                self.split = final_split\n",
    "\n",
    "                final_X_lower = X[X[:,self.split_characteristic]<=self.split, :]\n",
    "                final_X_higher = X[X[:,self.split_characteristic]>self.split, :]\n",
    "                final_y_lower = y[X[:,self.split_characteristic]<=self.split]\n",
    "                final_y_higher = y[X[:,self.split_characteristic]>self.split]\n",
    "                #assign children\n",
    "                if current_depth<max_depth:\n",
    "                    self.left = self.tree.Node_(X=final_X_lower,\\\n",
    "                                                y=final_y_lower,\\\n",
    "                                                tree=self.tree,\\\n",
    "                                                num_splits=num_splits,\\\n",
    "                                                max_depth=max_depth,\\\n",
    "                                                current_depth=current_depth+1)\n",
    "                    self.right = self.tree.Node_(X=final_X_higher,\\\n",
    "                                                y=final_y_higher,\\\n",
    "                                                tree=self.tree,\\\n",
    "                                                num_splits=num_splits,\\\n",
    "                                                max_depth=max_depth,\\\n",
    "                                                current_depth=current_depth+1)\n",
    "                    self.prediction_value = None\n",
    "                else:\n",
    "                    #value to predict with\n",
    "                    self.prediction_value = classes[np.argmax(np.array(p_classes))]\n",
    "    class __DecisionTreeRegressor__():\n",
    "        def __init__(self,X:np.ndarray,y:np.ndarray,max_depth,num_splits):\n",
    "            \"Constructs a Decision Tree which uses the sum of variance to choose the best splits\"\n",
    "            self.tree = self.Node_(X,y,max_depth,num_splits,self,0)\n",
    "\n",
    "        def predict(self,x):\n",
    "            \"Predicts using the established tree\"\n",
    "            node = self.tree\n",
    "            while node.prediction_value is None:\n",
    "                #decend tree\n",
    "                if x[node.split_characteristic] <= node.split: node = node.left\n",
    "                else: node = node.right\n",
    "            return node.prediction_value\n",
    "        class Node_():\n",
    "            def __init__(self, X,y, max_depth, num_splits, tree=None,current_depth=0):\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                self.tree = tree\n",
    "                #calculate entropy\n",
    "                classes = np.unique(y)\n",
    "                if len(classes) == 1:\n",
    "                    self.prediction_value = classes[0]\n",
    "                    return #early stop\n",
    "                p_classes = []\n",
    "                for class_ in classes:\n",
    "                    p_class = np.sum(y==class_)/len(y)\n",
    "                    p_classes.append(p_class)\n",
    "\n",
    "                variances = [] # used to find best split\n",
    "                total_splits = []\n",
    "                for column_index in range(X.shape[1]):\n",
    "                    curr_column = X[:,column_index]\n",
    "\n",
    "                    #pick 10 random potential split points\n",
    "                    #TODO: make this related to the column's value's range in a smart way\n",
    "                    random_splits = np.random.random_sample(num_splits,)*(curr_column.max()-curr_column.min())\\\n",
    "                                    + curr_column.min()\n",
    "                    total_splits = np.concatenate([total_splits, random_splits],axis=0)\n",
    "    \n",
    "                    # decide on split using information gain\n",
    "                    for split in random_splits:\n",
    "\n",
    "                        y_lower = y[curr_column<=split]\n",
    "                        y_higher = y[curr_column>split]\n",
    "                        y_lower_mean = y_lower.mean()\n",
    "                        y_higher_mean = y_higher.mean()\n",
    "         \n",
    "        \n",
    "                        variances.append(((y_lower-y_lower_mean)**2).sum()+((y_higher-y_higher_mean)**2).sum())\n",
    "\n",
    "                # split using best splitpoint\n",
    "                arg_min = np.argmin(np.array(variances))\n",
    "\n",
    "                self.split_characteristic = arg_min // len(random_splits)\n",
    "\n",
    "                final_split = total_splits[arg_min]\n",
    "\n",
    "                self.split = final_split\n",
    "\n",
    "                final_X_lower = X[X[:,self.split_characteristic]<=self.split, :]\n",
    "                final_X_higher = X[X[:,self.split_characteristic]>self.split, :]\n",
    "                final_y_lower = y[X[:,self.split_characteristic]<=self.split]\n",
    "                final_y_higher = y[X[:,self.split_characteristic]>self.split]\n",
    "                #assign children\n",
    "                if current_depth<max_depth:\n",
    "                    self.left = self.tree.Node_(final_X_lower,\\\n",
    "                                                final_y_lower,\\\n",
    "                                                max_depth,\\\n",
    "                                                num_splits,\\\n",
    "                                                self.tree,\\\n",
    "                                                current_depth+1)\n",
    "                    self.right = self.tree.Node_(final_X_higher,\\\n",
    "                                                 final_y_higher,\\\n",
    "                                                 max_depth,\\\n",
    "                                                 num_splits,\\\n",
    "                                                 self.tree,\\\n",
    "                                                 current_depth+1)\n",
    "                    self.prediction_value = None\n",
    "                else:\n",
    "                    #value to predict with\n",
    "                    self.prediction_value = y.mean()\n",
    "\n",
    "    def __is_continuous__(self,x):\n",
    "\n",
    "        if type(x[0]) in [np.float64,np.float,np.float128,np.float16,np.float32]: return True\n",
    "        # Enough ints that we can consider them continuous\n",
    "        elif type(x[0]) == np.int64 and len(np.unique(x[0])) > 10: return True\n",
    "        else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T20:20:53.161615Z",
     "start_time": "2019-05-24T20:20:53.138153Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T20:20:53.539605Z",
     "start_time": "2019-05-24T20:20:53.519699Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dataset['data']\n",
    "target = dataset['target']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T20:21:04.210935Z",
     "start_time": "2019-05-24T20:20:54.015118Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:111: RuntimeWarning: divide by zero encountered in log2\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:111: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:110: RuntimeWarning: divide by zero encountered in log2\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:110: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "forest =RandomForrest(data_train,target_train,num_trees=100,max_depth=5,num_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:04.456230Z",
     "start_time": "2019-05-24T08:05:04.420044Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = forest.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:04.464442Z",
     "start_time": "2019-05-24T08:05:04.458518Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#print recall as this is critical for medical tests\n",
    "\n",
    "TP = np.array([prediction and target for prediction,target in zip(predictions,target_test)])\n",
    "FN = np.array([ target and not prediction  for prediction,target in zip(predictions,target_test)])\n",
    "print(TP.sum()/(FN.sum()+TP.sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Really? That seems too good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:04.474017Z",
     "start_time": "2019-05-24T08:05:04.467389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:04.483472Z",
     "start_time": "2019-05-24T08:05:04.477632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FN.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thats good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets regress...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:04.497845Z",
     "start_time": "2019-05-24T08:05:04.487466Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:11.002561Z",
     "start_time": "2019-05-24T08:05:10.998942Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dataset['data']\n",
    "target = dataset['target']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:12.092843Z",
     "start_time": "2019-05-24T08:05:11.760080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:158: RuntimeWarning: Mean of empty slice.\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "regression_forrest = RandomForest(data_train,target_train,max_depth=7,num_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:13.057479Z",
     "start_time": "2019-05-24T08:05:13.052701Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.array(regression_forrest.predict(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:24.107047Z",
     "start_time": "2019-05-24T08:05:24.101555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.41      , 25.76666667, 15.44375   , 23.41      , 15.58      ,\n",
       "       20.33658537, 20.96363636, 19.3       , 20.33658537, 18.00416667])"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:34.590209Z",
     "start_time": "2019-05-24T08:05:34.585022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.6, 32.4, 13.6, 22.8, 16.1, 20. , 17.8, 14. , 19.6, 16.8])"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:50.977305Z",
     "start_time": "2019-05-24T08:05:50.973843Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:05:56.904903Z",
     "start_time": "2019-05-24T08:05:56.899937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.17451755167276"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions,target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:06:06.335549Z",
     "start_time": "2019-05-24T08:06:06.330762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.600000000000001"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(predictions-target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:06:11.790907Z",
     "start_time": "2019-05-24T08:06:11.785947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.799999999999997"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(predictions-target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again it seems we've made some errors but they are less than with one decision tree due to lower variance (due to the voting of the trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
