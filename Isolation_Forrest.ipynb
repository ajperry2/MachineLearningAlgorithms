{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-01T19:30:51.516497Z",
     "start_time": "2019-06-01T19:30:50.595838Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-01T19:31:22.356695Z",
     "start_time": "2019-06-01T19:31:22.345651Z"
    }
   },
   "source": [
    "# Isolation Forest Implementation\n",
    "\n",
    "Here I have implemented the Isolation forest with some personal customizations.\n",
    "\n",
    "I have added preprocessing to remove noisy features by ranking columns by the average splitting benefit of choosing a particular column\n",
    "\n",
    "I have sacrificed space for training speed. I cached the average depth of each point in a sample size x num trees matrix. This implementation may not be appropriate if you are using a large sample size. This is the beauty of this algorithm though, there is a degradation of return for adding data to your training set, to the point that even with 256 training data points, you can get good results.\n",
    "## The Algorithm\n",
    "All math equations, pictures, and model architecture is taken from this [algorithm's paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest)\n",
    "\n",
    "\n",
    "The Isolation tree makes a series of isolation trees. Each with a fixed maximum depth.\n",
    "\n",
    "![IForest](images/iForest.png)\n",
    "\n",
    "Each single isolation tree takes your data. Then it chooses a random feature and splits your data at a random split point(I limited this to the range between the max and min of the feature). This works because anomalies will tend not to reach the bottom of this tree if we pick a good maximum depth.\n",
    "\n",
    "![iTree](images/iTree.png)\n",
    "\n",
    "Inference is done by feeding a particular datapoint into each of the trees and observing the average depth across all trees. Then assigning it a score based off of this average depth(equation is below) and if this score is above a threshold you choose the data point is an anomaly. The range of these scores is roughly from \\frac{1}{2} to $2$. Depending how you count depths starting at 0 or 1.\n",
    "\n",
    "![iTree](images/score.png)\n",
    "\n",
    "\n",
    "## The API\n",
    "```\n",
    "class IsolationTreeEnsemble:\n",
    "    def __init__(self, sample_size, n_trees=10):\n",
    "        \n",
    "    def fit(self, X:np.ndarray, improved=False):\n",
    "        Given a 2D matrix of observations, create IsolationTree objects and store them \n",
    "\n",
    "    def path_length(self, X:np.ndarray) -> np.ndarray:\n",
    "        Given a 2D matrix of observations, X, compute the average path length\n",
    "        for each observation in X.  Compute the path length for x_i using every\n",
    "        tree in self.trees then compute the average for each x_i.  Return an\n",
    "        ndarray of shape (len(X),1).\n",
    "\n",
    "    def anomaly_score(self, X:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given a 2D matrix of observations, X, compute the anomaly score\n",
    "        for each x_i observation, returning an ndarray of them.\n",
    "        \"\"\"\n",
    "\n",
    "    def predict_from_anomaly_scores(self, scores:np.ndarray, threshold:float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given an array of scores and a score threshold, return an array of\n",
    "        the predictions: 1 for any score >= the threshold and 0 otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, X:np.ndarray, threshold:float) -> np.ndarray:\n",
    "        \"A shorthand for calling anomaly_score() and predict_from_anomaly_scores().\"\n",
    "\n",
    "class IsolationTree:\n",
    "    def __init__(self, height_limit):\n",
    "\n",
    "    def fit(self, X:np.ndarray, improved=False):\n",
    "        \"\"\"\n",
    "        Given a 2D matrix of observations, create an isolation tree. Set field\n",
    "        self.root to the root of that tree and return it.\n",
    "\n",
    "        If you are working on an improved algorithm, check parameter \"improved\"\n",
    "        and switch to your new functionality else fall back on your original code.\n",
    "        \"\"\"\n",
    "        return self.root\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Scoring Results\n",
    "\n",
    "These results were attained when 5 randomly generated columns were added to well known kaggle datasets to make our.\n",
    "\n",
    "| Kaggle Dataset   |  Number of Trees |    True Positive Rate      |  False Positive Rate |\n",
    "|----------|:-------------:|:-------------:|------:|\n",
    "| creditcard |300|  80% | 0.0122% |\n",
    "| http |300|    99%   |   0.0240% |\n",
    "| cancer |1000 |75% |    0.0924% |\n",
    "\n",
    "## Visible Results\n",
    "\n",
    "Here is a representation of our algorithm's results. Though the data has been flattened to be graphical. These are not associated with the above tests.\n",
    "\n",
    "### *Credit Card*\n",
    "![Credit Card](images/creditcard-200-80.svg)\n",
    "### *Cancer*\n",
    "![Cancer](images/cancer-300-80.svg)\n",
    "### *Http*\n",
    "\n",
    "![Http](images/http-200-99.svg)\n",
    "\n",
    "Notice how unintuitive the boundary is? This could be the result of flattening complicated data to 1 demension. The important thing is our model is good at predicting in the feature space that our data rests in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-01T19:32:23.764818Z",
     "start_time": "2019-06-01T19:32:23.729809Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class IsolationTreeEnsemble:\n",
    "    '''\n",
    "    A strong learner for anomaly detection\n",
    "    \n",
    "    This is a very fast model and does a very good job. I use this as a feature generator often,\n",
    "    as this result is very useful to other models. \n",
    "    \n",
    "    The Algorithm:\n",
    "        We take our features and select a random feature and then random split point. We hypothesize\n",
    "        that if a point is an outlier then it will tend to be isolated more often this way. So we score\n",
    "        points based off of their tendency to be isolated quickly.\n",
    "        \n",
    "        The canonical paper can be found here:\n",
    "        https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest\n",
    "    \n",
    "    Customization:\n",
    "        For my purposes I usually have columns which I do not want considered as they \"Water down\"\n",
    "        my results. I have added some preprocessing which has shown to improve results in the presence of\n",
    "        noisy columns. This does add to training time, but it can easily by setting the 'improved' \n",
    "        parameter to False\n",
    "    '''\n",
    "    def __init__(self, sample_size, n_trees=10):\n",
    "        '''\n",
    "        Parameters:\n",
    "        \n",
    "        sample_size: A real number which impacts the scoring of points and the amount of data in \n",
    "                     training. The paper mentioned they found empirically that 256 is often enough\n",
    "                     to get reliable results.\n",
    "        \n",
    "        n_trees: The number of weak learners to include. Higher numbers can lead lead to overfitting\n",
    "                 and slower training. Only crank this number up for large data!\n",
    "        '''\n",
    "        self.trees = []\n",
    "        self.n_trees = n_trees\n",
    "        self.sample_size = np.array(sample_size)\n",
    "        self.c = 2 * (np.log2(self.sample_size-1)+ 0.5772156649)-(2*(self.sample_size-1)/self.sample_size)\n",
    "\n",
    "    def fit(self, X:np.ndarray, improved=False):\n",
    "        \"\"\"\n",
    "        Given a 2D matrix of observations, create an ensemble of IsolationTree\n",
    "        objects and store them in a list: self.trees.  Convert DataFrames to\n",
    "        ndarray objects.\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        #    Adjusting for noise\n",
    "        # 1. Find six random split points for each column.\n",
    "        # 2. Add up the minumum split size.\n",
    "        # 3. Remove the columns with the largest sums.\n",
    "        if improved:\n",
    "            Y = X.T\n",
    "            columns = np.array(range(Y.shape[0]))\n",
    "            good_cols = []\n",
    "            mins = None\n",
    "            maxs = None\n",
    "            mins = [column.min() for column in Y]\n",
    "            maxs = [column.max() for column in Y]\n",
    "            orig_col_len = len(columns)\n",
    "            # This needs to be customized based on the number of columns in dataset\n",
    "            if orig_col_len >20: \n",
    "                while len(columns)>0:\n",
    "                    column_split_potential = []\n",
    "                    #indeces of 3 random columns\n",
    "                    cols = np.random.choice(columns,size=7)\n",
    "                    \n",
    "                    for col in cols:\n",
    "                        column = Y[col]\n",
    "                        sum_ = 0\n",
    "                        splits= np.random.uniform(mins[col],maxs[col],size=40)\n",
    "\n",
    "                        X1=[(column < split).sum() for split in splits]\n",
    "                        X2=[(column >= split).sum() for split in splits]\n",
    "                        least_partitions = [ min(x1,x2) for x1,x2 in zip(X1,X2)]\n",
    "\n",
    "                        column_split_potential.append(sum(least_partitions))\n",
    "                    # Keep minimum\n",
    "                    good_cols.append(cols[np.argmin(column_split_potential)])\n",
    "                    temp_max = cols[int(np.array(column_split_potential).argmax())]\n",
    "                    columns = columns[columns!=cols[np.argmin(column_split_potential)]]\n",
    "\n",
    "                    columns = columns[columns!=temp_max]\n",
    "            else:\n",
    "                while len(columns)>0:\n",
    "                    column_split_potential = []\n",
    "                    #indeces of 3 random columns\n",
    "                    cols = np.random.choice(columns,size=3)\n",
    "\n",
    "                    for col in cols:\n",
    "                        column = Y[col]\n",
    "                        sum_ = 0\n",
    "                        splits= np.random.uniform(mins[col],maxs[col],size=40)\n",
    "\n",
    "                        X1=[(column < split).sum() for split in splits]\n",
    "                        X2=[(column >= split).sum() for split in splits]\n",
    "                        least_partitions = [ min(x1,x2) for x1,x2 in zip(X1,X2)]\n",
    "\n",
    "                        column_split_potential.append(sum(least_partitions))\n",
    "                    # Keep minimum\n",
    "                    good_cols.append(cols[np.argmin(column_split_potential)])\n",
    "                    temp_max = cols[int(np.array(column_split_potential).argmax())]\n",
    "                    columns = columns[columns!=cols[np.argmin(column_split_potential)]]\n",
    "\n",
    "                    columns = columns[columns!=temp_max]\n",
    "            #prune good cols\n",
    "            if len(good_cols) > 10:\n",
    "\n",
    "                sums_ = []\n",
    "\n",
    "                for col in list(set(good_cols)):\n",
    "                    column = Y[col]\n",
    "                    splits= np.random.uniform(mins[col],maxs[col],size=20)\n",
    "\n",
    "                    X1=[(column < split).sum() for split in splits]\n",
    "                    X2=[(column >= split).sum() for split in splits]\n",
    "                    least_partitions = [ min(x1,x2) for x1,x2 in zip(X1,X2)]\n",
    "                    sums_.append(sum(least_partitions))\n",
    "                sums_= np.array(list(set(sums_)))\n",
    "                temp_max = np.array(list(set(good_cols)))[sums_.argsort()[:-2]]\n",
    "                good_cols = temp_max\n",
    "            good_cols = list(set(int(x) for x in good_cols))\n",
    "            print(good_cols)\n",
    "            Y= Y[good_cols]\n",
    "            X=Y.T\n",
    "        # Maximum height of a tree.\n",
    "        height_limit = np.ceil(np.log2(self.sample_size))\n",
    "        \n",
    "        for tree_num in range(self.n_trees):\n",
    "            # Sample data.\n",
    "            np.random.seed(seed=2*tree_num)\n",
    "            data_index = np.random.randint(low=0,high=X.shape[0],size=self.sample_size)\n",
    "            sample = X[data_index,:]\n",
    "            \n",
    "            # Fit tree, initialize parameters, add to list\n",
    "            new_tree = IsolationTree(sample,0,height_limit,self.sample_size)\n",
    "            root = new_tree.fit(sample)\n",
    "            new_tree.root = root\n",
    "            new_tree.n_nodes = new_tree.root.n_nodes\n",
    "            self.trees.append(new_tree)\n",
    "        self.depth_matrix = np.zeros(shape=(X.shape[0],self.n_trees),dtype=float)\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def anomaly_score(self,X:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given a 2D matrix of observations, X, compute the anomaly score\n",
    "        for each x_i observation, returning an ndarray of them.\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        for tree_idx in range(self.n_trees):\n",
    "            self.trees[tree_idx].find_depth(indeces=np.array(range(X.shape[0])),\\\n",
    "                                            X=X,\\\n",
    "                                            forrest=self,\\\n",
    "                                            tree_index=tree_idx,\\\n",
    "                                           node = self.trees[tree_idx].root)\n",
    "        return np.power(2, -self.depth_matrix.mean(axis=1)/self.c)\n",
    "    \n",
    "    def predict_from_anomaly_scores(self, scores:np.ndarray, threshold:float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given an array of scores and a score threshold, return an array of\n",
    "        the predictions: 1 for any score >= the threshold and 0 otherwise.\n",
    "        \"\"\"\n",
    "        result = np.zeros(scores.shape)\n",
    "        result[scores>threshold] = 1\n",
    "        return result\n",
    "    def predict(self, X:np.ndarray, threshold:float) -> np.ndarray:\n",
    "        \"A shorthand for calling anomaly_score() and predict_from_anomaly_scores().\"\n",
    "        scores = anomaly_score(X)\n",
    "        return predict_from_anomaly_scores(scores,threshold)\n",
    "    \n",
    "def find_TPR_threshold(y, scores, desired_TPR):\n",
    "    for i in np.linspace(start=1,stop=0,num=1000):\n",
    "        result = np.zeros(scores.shape)\n",
    "        result[scores>i] = 1\n",
    "        con_mat = confusion_matrix(y, result)\n",
    "        TN, FP, FN, TP= con_mat.flat\n",
    "        TP_Rate = TP / (TP + FN)\n",
    "        FP_Rate = FP / (FP + TN)\n",
    "        if TP_Rate > desired_TPR:\n",
    "            return i, FP_Rate\n",
    "\n",
    "class IsolationTree:\n",
    "    def __init__(self, X:np.ndarray,current_height,height_limit,sample_size):\n",
    "        self.height_limit = height_limit\n",
    "        self.current_height = current_height\n",
    "        self.sample_size=sample_size\n",
    "        self.root = None\n",
    "        self.n_nodes = None\n",
    "    def fit(self, X:np.ndarray, improved=False):\n",
    "        \"\"\"\n",
    "        Given a 2D matrix of observations, create an isolation tree. Set field\n",
    "        self.root to the root of that tree and return it.\n",
    "\n",
    "        If you are working on an improved algorithm, check parameter \"improved\"\n",
    "        and switch to your new functionality else fall back on your original code.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.build_tree(X,0)\n",
    "        \n",
    "    def build_tree(self,X,current_height=0):\n",
    "        \n",
    "        if X.shape[0] <=1 or current_height >= self.height_limit:\n",
    "            return exNode(size = X.shape[0],depth=np.array(current_height))\n",
    "\n",
    "        # Randomly select attribute index.\n",
    "        q = np.random.randint(low=0,high=X.shape[1])\n",
    "\n",
    "        # Randomly select split value.\n",
    "        r = np.random.random()\n",
    "        min_q = X[:,q].min()\n",
    "        max_q = X[:,q].max()\n",
    "        split = min_q + r * (max_q - min_q) \n",
    "        \n",
    "        # Conditions of returning terminating node are split up so we do not need to compute max each time.\n",
    "        if min_q == max_q : return exNode(size = X.shape[0],depth=np.array(current_height))\n",
    "\n",
    "        # Split X  on q attribute.\n",
    "        left_index = (X[:,q] < split )\n",
    "        x1=X[left_index ]\n",
    "        x2=X[np.invert(left_index)]            \n",
    "        \n",
    "        # Make children...\n",
    "        parent = inNode(\n",
    "            left=IsolationTree(x1,current_height+1,self.height_limit,self.sample_size)\n",
    "                    .build_tree(x1,current_height+1),\n",
    "            right=IsolationTree(x2,current_height+1,self.height_limit,self.sample_size)\n",
    "                    .build_tree(x2,current_height+1),\n",
    "            splitAtt=q,\n",
    "            splitVal=split)\n",
    "        return parent  \n",
    "\n",
    "\n",
    "    def find_depth(self,indeces,X,forrest,tree_index,node):\n",
    "        '''\n",
    "        A helper function which returns the depth in a tree which a value appears.\n",
    "        '''\n",
    "\n",
    "        if not isinstance(node,exNode):\n",
    "            #update relevant matrix\n",
    "            X_left = X[:,node.splitAtt] < node.splitVal\n",
    "            X_right = np.invert(X_left)\n",
    "            #descend\n",
    "            left = node.left\n",
    "            self.find_depth(indeces[X_left],X[X_left],forrest,tree_index,left)\n",
    "            right = node.right\n",
    "            self.find_depth(indeces[X_right],X[X_right],forrest,tree_index,right)\n",
    "        else:        #pass relevant X vals\n",
    "            forrest.depth_matrix[indeces,tree_index] = node.depth\n",
    "class inNode:\n",
    "    def __init__(self,left,right,splitAtt,splitVal):\n",
    "        self.right = right\n",
    "        self.left = left\n",
    "        self.splitAtt = splitAtt\n",
    "        self.splitVal = splitVal\n",
    "        self.n_nodes=0\n",
    "        # Parameter n_nodes is passed up to the root to know how many nodes are in a tree.\n",
    "        if isinstance(left,exNode) and isinstance(right,exNode) :\n",
    "            self.n_nodes = 2 + 1\n",
    "        elif isinstance(left,exNode) and isinstance(right,inNode) :\n",
    "            self.n_nodes = 2 + right.n_nodes\n",
    "        elif isinstance(left,inNode) and isinstance(right,exNode) :\n",
    "            self.n_nodes = left.n_nodes + 2\n",
    "        elif isinstance(left,inNode) and isinstance(right,inNode) :\n",
    "            self.n_nodes = left.n_nodes + right.n_nodes + 1\n",
    "\n",
    "\n",
    "class exNode:\n",
    "    def __init__(self,size,depth):\n",
    "        self.depth = depth\n",
    "        self.size = size\n",
    "        self.n_nodes = 1\n",
    "        if size >2:\n",
    "            self.c = 2 * (np.log2(size-1)+0.577215)-(2*(size-1)/size)\n",
    "        else: # Must have at least one value\n",
    "            self.c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
