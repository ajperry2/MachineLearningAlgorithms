{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "### A series of trees that learn sequentially to predict better.\n",
    "\n",
    "This model uses a series of weights which determine how difficult a datapoint is to classify. These weights are updated for the weights associated with the errors and effect the training of all future trees. The thinking behind this is, if the trees get better over time. Through the relative weighting of the trees based on how good they are, we will get better results than any individual tree.\n",
    "\n",
    "[The psuedocode of AdaBoost](./images/The-pseudocode-of-the-AdaBoost-algorithm.png)\n",
    "\n",
    "\n",
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T18:23:48.786164Z",
     "start_time": "2019-05-28T18:23:48.774673Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:14:22.231488Z",
     "start_time": "2019-05-28T22:14:22.210251Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self,X:np.ndarray,y:np.ndarray,num_trees=1,num_splits=10):\n",
    "        '''\n",
    "        AdaBoost:\n",
    "        Parameters:\n",
    "            X: An array of numerical factors\n",
    "            y: The responding variable, can be categorical or continuous\n",
    "            num_trees: The number of estimators which are voting to form a prediction\n",
    "            max_depth: The maximum number of nodes from root to leaves in each estimator\n",
    "            num_splits: The number of splits to be tested for each factor at each node\n",
    "        '''\n",
    "        self.trees = []\n",
    "        self.__is_continuous__ = self.__is_continuous__(y)\n",
    "        self.weights=np.ones(shape=(X.shape[0])) / len(X)\n",
    "        self.alphas = []\n",
    "        for tree in range(num_trees):\n",
    "            new_tree=self.__DecisionTreeClassifier__(X,y,5,num_splits)\n",
    "            curr_x = np.multiply(X.T,self.weights).T\n",
    "            current_predictions = new_tree.predict(curr_x)\n",
    "\n",
    "            errors = (np.array(current_predictions) != np.array(y))\n",
    "            print(errors.shape)\n",
    "            err = self.weights[errors].sum()/self.weights.sum()\n",
    "\n",
    "            alpha = np.log2((1-err)/(err+0.00001))\n",
    "\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            self.weights[errors]*=np.exp(alpha)\n",
    "\n",
    "            self.trees.append(new_tree)\n",
    "        self.alphas = np.array(self.alphas)\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Predict:\n",
    "        Parameters:\n",
    "            X: An array of numerical factors\n",
    "            \n",
    "        Returns:\n",
    "            A series of predictions, the continuity of predictions depends on the labels the model was trained on\n",
    "        '''\n",
    "        predictions = []\n",
    "        for tree in self.trees:\n",
    "            predictions.append(tree.predict(X))\n",
    "        predictions = np.dot(np.array(predictions).T,self.alphas) \n",
    "        return np.array(predictions).sum()>0.5\n",
    "    class __DecisionTreeClassifier__():\n",
    "        def __init__(self,X:np.ndarray,y:np.ndarray,max_depth,num_splits):\n",
    "            '''\n",
    "            Decision Tree Classifier:\n",
    "            Parameters:\n",
    "                X: An array of numerical factors\n",
    "                y: The responding variable, must be categorical\n",
    "                max_depth: The maximum number of nodes from root to leaves in each estimator\n",
    "                num_splits: The number of splits to be tested for each factor at each node\n",
    "            '''\n",
    "            self.tree = self.Node_(X,y,self,num_splits,max_depth)\n",
    "        def predict(self,X):\n",
    "            '''\n",
    "            Predict:\n",
    "            Predicts using the established tree\n",
    "\n",
    "            Parameters:\n",
    "                X: An array of numerical factors\n",
    "\n",
    "            Returns:\n",
    "                An array of predictions.\n",
    "            '''\n",
    "            total_predictions=[]\n",
    "            for x in X:\n",
    "                node = self.tree\n",
    "                while node.prediction_value is None:\n",
    "                    #decend tree\n",
    "                    if x[node.split_characteristic] <= node.split: node = node.left\n",
    "                    else: node = node.right\n",
    "                total_predictions.append(node.prediction_value)\n",
    "            return total_predictions\n",
    "        class Node_():\n",
    "            def __init__(self, X,y,tree, num_splits, max_depth,current_depth=0):\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "\n",
    "                self.tree = tree\n",
    "                #calculate entropy\n",
    "                classes = np.unique(y)\n",
    "                if len(classes) == 1:\n",
    "                    self.prediction_value = classes[0]\n",
    "                    return #early stop\n",
    "                p_classes = []\n",
    "                for class_ in classes:\n",
    "                    p_class = np.sum(y==class_)/len(y)\n",
    "                    p_classes.append(p_class)\n",
    "\n",
    "                self.entropy = -np.sum(np.array([p_class*np.log2(p_class) for p_class in p_classes]))\n",
    "\n",
    "\n",
    "                info_gains = [] # used to find best split\n",
    "                total_splits = []\n",
    "\n",
    "                for column_index in range(X.shape[1]):\n",
    "                    curr_column = X[:,column_index]\n",
    "\n",
    "                    #pick 10 random potential split points\n",
    "\n",
    "                    random_splits = np.random.random_sample(num_splits,)*(curr_column.max()-curr_column.min())\\\n",
    "                                    + curr_column.min()\n",
    "                    total_splits = np.concatenate([total_splits, random_splits],axis=0)\n",
    "\n",
    "                    # decide on best split using information gain\n",
    "                    for split in random_splits:\n",
    "\n",
    "                        y_lower = y[curr_column<=split]\n",
    "                        y_higher = y[curr_column>split]\n",
    "                        lower_p_classes = []\n",
    "                        higher_p_classes = []\n",
    "                        #find entropy of each split\n",
    "                        for class_ in classes:\n",
    "                            lower_p_class = np.sum(y_lower==class_)/len(y_lower)\n",
    "                            lower_p_classes.append(lower_p_class)    \n",
    "                            higher_p_class = np.sum(y_higher==class_)/len(y_higher)\n",
    "                            higher_p_classes.append(higher_p_class)    \n",
    "                        lower_entropy = -np.sum(np.array([p_class*np.log2(p_class) for p_class in lower_p_classes]))\n",
    "                        higher_entropy = -np.sum(np.array([p_class*np.log2(p_class) for p_class in higher_p_classes]))\n",
    "\n",
    "                        info_gains.append(self.entropy -  higher_entropy - lower_entropy)\n",
    "                # split using best splitpoint\n",
    "                arg_max = np.argmax(np.array(info_gains))\n",
    "                self.split_characteristic = arg_max // len(random_splits)\n",
    "                final_split = total_splits[arg_max]\n",
    "                self.split = final_split\n",
    "                # also split X and y\n",
    "                final_X_lower = X[X[:,self.split_characteristic]<=self.split, :]\n",
    "                final_X_higher = X[X[:,self.split_characteristic]>self.split, :]\n",
    "                final_y_lower = y[X[:,self.split_characteristic]<=self.split]\n",
    "                final_y_higher = y[X[:,self.split_characteristic]>self.split]\n",
    "                #assign children\n",
    "                if current_depth<max_depth:\n",
    "                    self.left = self.tree.Node_(X=final_X_lower,\\\n",
    "                                                y=final_y_lower,\\\n",
    "                                                tree=self.tree,\\\n",
    "                                                num_splits=num_splits,\\\n",
    "                                                max_depth=max_depth,\\\n",
    "                                                current_depth=current_depth+1)\n",
    "                    self.right = self.tree.Node_(X=final_X_higher,\\\n",
    "                                                y=final_y_higher,\\\n",
    "                                                tree=self.tree,\\\n",
    "                                                num_splits=num_splits,\\\n",
    "                                                max_depth=max_depth,\\\n",
    "                                                current_depth=current_depth+1)\n",
    "                    self.prediction_value = None\n",
    "                #asign property to be predicted\n",
    "                else:\n",
    "                    self.prediction_value = classes[np.argmax(np.array(p_classes))]\n",
    "\n",
    "\n",
    "    def __is_continuous__(self,x):\n",
    "\n",
    "        if type(x[0]) in [np.float64,np.float,np.float128,np.float16,np.float32]: return True\n",
    "        # Enough ints that we can consider them continuous\n",
    "        elif type(x[0]) == np.int64 and len(np.unique(x[0])) > 10: return True\n",
    "        else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T20:23:03.987082Z",
     "start_time": "2019-05-24T20:23:03.901477Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:14:22.864466Z",
     "start_time": "2019-05-28T22:14:22.846293Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:14:23.076659Z",
     "start_time": "2019-05-28T22:14:23.072889Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dataset['data']\n",
    "target = dataset['target']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:14:28.735700Z",
     "start_time": "2019-05-28T22:14:23.581468Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:121: RuntimeWarning: divide by zero encountered in log2\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:121: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:122: RuntimeWarning: divide by zero encountered in log2\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:122: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n",
      "(455,)\n"
     ]
    }
   ],
   "source": [
    "ADA =AdaBoost(data_train,target_train,num_trees=50,num_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:14:29.147401Z",
     "start_time": "2019-05-28T22:14:29.131446Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = ADA.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:14:29.567837Z",
     "start_time": "2019-05-28T22:14:29.562745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.58953059e-01, -3.35971771e-01,  1.48728473e-01, -6.58391472e-02,\n",
       "        2.91457259e-02, -1.29022455e-02,  5.71157440e-03, -2.52840317e-03,\n",
       "        1.11927509e-03, -4.95481379e-04,  2.19340002e-04, -9.70975665e-05,\n",
       "        4.29832104e-05, -1.90278339e-05,  8.42325321e-06, -3.72881090e-06,\n",
       "        1.65067230e-06, -7.30720627e-07,  3.23475855e-07, -1.43196490e-07,\n",
       "        6.33903104e-08, -2.80616609e-08,  1.24223523e-08, -5.49913505e-09,\n",
       "        2.43436068e-09, -1.07764405e-09,  4.77052032e-10, -2.11181888e-10,\n",
       "        9.34862363e-11, -4.13849068e-11,  1.83203962e-11, -8.11059539e-12,\n",
       "        3.59072077e-12, -1.58921989e-12,  7.03792803e-13, -3.12334084e-13,\n",
       "        1.38388025e-13, -6.02244183e-14,  2.59477547e-14, -1.15323354e-14,\n",
       "        4.80513976e-15, -2.88308385e-15,  2.88308385e-15, -2.88308385e-15,\n",
       "        2.88308385e-15, -2.88308385e-15,  2.88308385e-15, -2.88308385e-15,\n",
       "        2.88308385e-15, -2.88308385e-15])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADA.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:14:34.996127Z",
     "start_time": "2019-05-28T22:14:34.973225Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "89\n",
      "86\n",
      "82\n",
      "87\n",
      "85\n",
      "87\n",
      "86\n",
      "86\n",
      "80\n",
      "89\n",
      "88\n",
      "89\n",
      "86\n",
      "90\n",
      "87\n",
      "86\n",
      "88\n",
      "86\n",
      "86\n",
      "83\n",
      "90\n",
      "86\n",
      "85\n",
      "86\n",
      "80\n",
      "83\n",
      "87\n",
      "73\n",
      "76\n",
      "86\n",
      "80\n",
      "88\n",
      "83\n",
      "95\n",
      "82\n",
      "87\n",
      "85\n",
      "89\n",
      "89\n",
      "86\n",
      "87\n",
      "88\n",
      "90\n",
      "90\n",
      "83\n",
      "72\n",
      "85\n",
      "87\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "for tree in ADA.trees:\n",
    "    predictions = tree.predict(data_test)\n",
    "    TP = np.array([prediction == target for prediction,target in zip(predictions,target_test)])\n",
    "    print(TP.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:14:41.643000Z",
     "start_time": "2019-05-28T22:14:41.638102Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7807017543859649\n"
     ]
    }
   ],
   "source": [
    "#print recall as this is critical for medical tests\n",
    "\n",
    "TP = np.array([prediction == target for prediction,target in zip(predictions,target_test)])\n",
    "FN = np.array([ target != prediction  for prediction,target in zip(predictions,target_test)])\n",
    "\n",
    "\n",
    "print(TP.sum()/(FN.sum()+TP.sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the best, but it's great for combining 5 decision stumps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T21:49:33.402754Z",
     "start_time": "2019-05-28T21:49:33.397516Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T21:49:41.328543Z",
     "start_time": "2019-05-28T21:49:41.324628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
