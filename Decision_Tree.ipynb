{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "### A decision tree has a series of nodes. \n",
    "\n",
    "### To make a prediction you start at the trees root node. If the node has children you decend to the child depending on how your input compares with this nodes split point. If the node does not have children for a regression tree you guess the mean, for a tree classifier you guess the most common class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The split point of a node in a decision is chosen such that information gain is maximized.\n",
    "\n",
    "Intuitively information gain can be see as how much we lower the combined spread of a distribution by splitting it at a specific location.\n",
    "\n",
    "$$H(X) = -\\sum p(X)\\log p(X)\\\\ Information\\ Gain\\; I(X,Y)= H(X)-H(X|Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:22:29.147871Z",
     "start_time": "2019-05-24T07:22:29.145282Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:49:00.235320Z",
     "start_time": "2019-05-24T07:49:00.219790Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self,X:np.ndarray,y:np.ndarray,max_depth,num_splits):\n",
    "        '''\n",
    "        Decision Tree Classifier:\n",
    "        Parameters:\n",
    "            X: An array of numerical factors\n",
    "            y: The responding variable, must be categorical\n",
    "            max_depth: The maximum number of nodes from root to leaves in each estimator\n",
    "            num_splits: The number of splits to be tested for each factor at each node\n",
    "        '''\n",
    "        self.tree = self.Node_(X,y,self,num_splits,max_depth)\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Predict:\n",
    "        Predicts using the established tree\n",
    "        \n",
    "        Parameters:\n",
    "            X: An array of numerical factors\n",
    "            \n",
    "        Returns:\n",
    "            An array of predictions.\n",
    "        '''\n",
    "        total_predictions=[]\n",
    "        for x in X:\n",
    "            node = self.tree\n",
    "            while node.prediction_value is None:\n",
    "                #decend tree\n",
    "                if x[node.split_characteristic] <= node.split: node = node.left\n",
    "                else: node = node.right\n",
    "            total_predictions.append(node.prediction_value)\n",
    "        return total_predictions\n",
    "    class Node_():\n",
    "        def __init__(self, X,y,tree, num_splits, max_depth,current_depth=0):\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "\n",
    "            self.tree = tree\n",
    "            #calculate entropy\n",
    "            classes = np.unique(y)\n",
    "            if len(classes) == 1:\n",
    "                self.prediction_value = classes[0]\n",
    "                return #early stop\n",
    "            p_classes = []\n",
    "            for class_ in classes:\n",
    "                p_class = np.sum(y==class_)/len(y)\n",
    "                p_classes.append(p_class)\n",
    "\n",
    "            self.entropy = -np.sum(np.array([p_class*np.log2(p_class) for p_class in p_classes]))\n",
    "\n",
    "\n",
    "            info_gains = [] # used to find best split\n",
    "            total_splits = []\n",
    "\n",
    "            for column_index in range(X.shape[1]):\n",
    "                curr_column = X[:,column_index]\n",
    "\n",
    "                #pick 10 random potential split points\n",
    "\n",
    "                random_splits = np.random.random_sample(num_splits,)*(curr_column.max()-curr_column.min())\\\n",
    "                                + curr_column.min()\n",
    "                total_splits = np.concatenate([total_splits, random_splits],axis=0)\n",
    "\n",
    "                # decide on best split using information gain\n",
    "                for split in random_splits:\n",
    "\n",
    "                    y_lower = y[curr_column<=split]\n",
    "                    y_higher = y[curr_column>split]\n",
    "                    lower_p_classes = []\n",
    "                    higher_p_classes = []\n",
    "                    #find entropy of each split\n",
    "                    for class_ in classes:\n",
    "                        lower_p_class = np.sum(y_lower==class_)/len(y_lower)\n",
    "                        lower_p_classes.append(lower_p_class)    \n",
    "                        higher_p_class = np.sum(y_higher==class_)/len(y_higher)\n",
    "                        higher_p_classes.append(higher_p_class)    \n",
    "                    lower_entropy = -np.sum(np.array([p_class*np.log2(p_class) for p_class in lower_p_classes]))\n",
    "                    higher_entropy = -np.sum(np.array([p_class*np.log2(p_class) for p_class in higher_p_classes]))\n",
    "\n",
    "                    info_gains.append(self.entropy -  higher_entropy - lower_entropy)\n",
    "            # split using best splitpoint\n",
    "            arg_max = np.argmax(np.array(info_gains))\n",
    "            self.split_characteristic = arg_max // len(random_splits)\n",
    "            final_split = total_splits[arg_max]\n",
    "            self.split = final_split\n",
    "            # also split X and y\n",
    "            final_X_lower = X[X[:,self.split_characteristic]<=self.split, :]\n",
    "            final_X_higher = X[X[:,self.split_characteristic]>self.split, :]\n",
    "            final_y_lower = y[X[:,self.split_characteristic]<=self.split]\n",
    "            final_y_higher = y[X[:,self.split_characteristic]>self.split]\n",
    "            #assign children\n",
    "            if current_depth<max_depth:\n",
    "                self.left = self.tree.Node_(X=final_X_lower,\\\n",
    "                                            y=final_y_lower,\\\n",
    "                                            tree=self.tree,\\\n",
    "                                            num_splits=num_splits,\\\n",
    "                                            max_depth=max_depth,\\\n",
    "                                            current_depth=current_depth+1)\n",
    "                self.right = self.tree.Node_(X=final_X_higher,\\\n",
    "                                            y=final_y_higher,\\\n",
    "                                            tree=self.tree,\\\n",
    "                                            num_splits=num_splits,\\\n",
    "                                            max_depth=max_depth,\\\n",
    "                                            current_depth=current_depth+1)\n",
    "                self.prediction_value = None\n",
    "            #asign property to be predicted\n",
    "            else:\n",
    "                self.prediction_value = classes[np.argmax(np.array(p_classes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T20:23:03.987082Z",
     "start_time": "2019-05-24T20:23:03.901477Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "        def __init__(self,X:np.ndarray,y:np.ndarray,max_depth,num_splits):\n",
    "            '''\n",
    "            Decision Tree Classifier:\n",
    "            Parameters:\n",
    "                X: An array of numerical factors\n",
    "                y: The responding variable, must be continuous\n",
    "                max_depth: The maximum number of nodes from root to leaves in each estimator\n",
    "                num_splits: The number of splits to be tested for each factor at each node\n",
    "            '''\n",
    "            self.tree = self.Node_(X,y,max_depth,num_splits,self,0)\n",
    "\n",
    "        def predict(self,X):\n",
    "            '''\n",
    "            Predict:\n",
    "            Predicts using the established tree\n",
    "\n",
    "            Parameters:\n",
    "                X: An array of numerical factors\n",
    "\n",
    "            Returns:\n",
    "                An array of predictions.\n",
    "            '''            \n",
    "            total_predictions=[]\n",
    "            for x in X:\n",
    "                node = self.tree\n",
    "                while node.prediction_value is None:\n",
    "                    #decend tree\n",
    "                    if x[node.split_characteristic] <= node.split: node = node.left\n",
    "                    else: node = node.right\n",
    "                total_predictions.append(node.prediction_value)\n",
    "            return total_predictions\n",
    "        class Node_():\n",
    "            def __init__(self, X,y, max_depth, num_splits, tree=None,current_depth=0):\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                self.tree = tree\n",
    "                #calculate entropy\n",
    "                classes = np.unique(y)\n",
    "                if len(classes) == 1:\n",
    "                    self.prediction_value = classes[0]\n",
    "                    return #early stopping if we have prematurely gotten a 'Pure Node'\n",
    "                # get the proportions of each class in y\n",
    "                p_classes = []\n",
    "                for class_ in classes:\n",
    "                    p_class = np.sum(y==class_)/len(y)\n",
    "                    p_classes.append(p_class)\n",
    "\n",
    "                variances = [] # used to find best split\n",
    "                total_splits = []\n",
    "                for column_index in range(X.shape[1]):\n",
    "                    curr_column = X[:,column_index]\n",
    "\n",
    "                    #pick 10 random potential split points\n",
    "                    random_splits = np.random.random_sample(num_splits,)*(curr_column.max()-curr_column.min())\\\n",
    "                                    + curr_column.min()\n",
    "                    total_splits = np.concatenate([total_splits, random_splits],axis=0)\n",
    "    \n",
    "                    # decide on best split using information gain\n",
    "                    for split in random_splits:\n",
    "                        #find combined mse for each split\n",
    "\n",
    "                        y_lower = y[curr_column<=split]\n",
    "                        y_higher = y[curr_column>split]\n",
    "                        y_lower_mean = y_lower.mean()\n",
    "                        y_higher_mean = y_higher.mean()\n",
    "         \n",
    "        \n",
    "                        variances.append(((y_lower-y_lower_mean)**2).sum()+((y_higher-y_higher_mean)**2).sum())\n",
    "\n",
    "                # split using best splitpoint\n",
    "                arg_min = np.argmin(np.array(variances))\n",
    "\n",
    "                self.split_characteristic = arg_min // len(random_splits)\n",
    "\n",
    "                final_split = total_splits[arg_min]\n",
    "\n",
    "                self.split = final_split\n",
    "\n",
    "                final_X_lower = X[X[:,self.split_characteristic]<=self.split, :]\n",
    "                final_X_higher = X[X[:,self.split_characteristic]>self.split, :]\n",
    "                final_y_lower = y[X[:,self.split_characteristic]<=self.split]\n",
    "                final_y_higher = y[X[:,self.split_characteristic]>self.split]\n",
    "                #assign children\n",
    "                if current_depth<max_depth:\n",
    "                    self.left = self.tree.Node_(final_X_lower,\\\n",
    "                                                final_y_lower,\\\n",
    "                                                max_depth,\\\n",
    "                                                num_splits,\\\n",
    "                                                self.tree,\\\n",
    "                                                current_depth+1)\n",
    "                    self.right = self.tree.Node_(final_X_higher,\\\n",
    "                                                 final_y_higher,\\\n",
    "                                                 max_depth,\\\n",
    "                                                 num_splits,\\\n",
    "                                                 self.tree,\\\n",
    "                                                 current_depth+1)\n",
    "                    self.prediction_value = None\n",
    "                else:\n",
    "                    #value to predict with\n",
    "                    self.prediction_value = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:36.610413Z",
     "start_time": "2019-05-24T07:58:36.594284Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:36.964952Z",
     "start_time": "2019-05-24T07:58:36.961463Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dataset['data']\n",
    "target = dataset['target']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:37.470231Z",
     "start_time": "2019-05-24T07:58:37.361417Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in log2\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: RuntimeWarning: divide by zero encountered in log2\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "tree =DecisionTreeClassifier(data_train,target_train,max_depth=4,num_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:38.153566Z",
     "start_time": "2019-05-24T07:58:38.150740Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = tree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:39.912116Z",
     "start_time": "2019-05-24T07:58:39.907838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#print recall as this is critical for medical tests\n",
    "\n",
    "TP = np.array([prediction and target for prediction,target in zip(predictions,target_test)])\n",
    "FN = np.array([ target and not prediction  for prediction,target in zip(predictions,target_test)])\n",
    "print(TP.sum()/(FN.sum()+TP.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:40.519906Z",
     "start_time": "2019-05-24T07:58:40.514440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:40.984957Z",
     "start_time": "2019-05-24T07:58:40.980869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:41.887046Z",
     "start_time": "2019-05-24T07:58:41.882880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is good, how is our precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:42.554027Z",
     "start_time": "2019-05-24T07:58:42.549804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6893203883495146\n"
     ]
    }
   ],
   "source": [
    "TP = np.array([prediction and target for prediction,target in zip(predictions,target_test)])\n",
    "FP = np.array([ prediction and not target  for prediction,target in zip(predictions,target_test)])\n",
    "print(TP.sum()/(FP.sum()+TP.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the best, but we can work on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:53:05.289289Z",
     "start_time": "2019-05-24T07:53:05.284611Z"
    }
   },
   "source": [
    "Lets try some regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:44.390178Z",
     "start_time": "2019-05-24T07:58:44.380995Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T07:58:44.838273Z",
     "start_time": "2019-05-24T07:58:44.834904Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dataset['data']\n",
    "target = dataset['target']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:01:39.553658Z",
     "start_time": "2019-05-24T08:01:39.122199Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: RuntimeWarning: Mean of empty slice.\n",
      "/Users/alan/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "regression_tree =DecisionTreeRegressor(data_train,target_train,max_depth=7,num_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:01:39.559143Z",
     "start_time": "2019-05-24T08:01:39.555772Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.array(regression_tree.predict(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:01:40.032617Z",
     "start_time": "2019-05-24T08:01:40.028058Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.3       , 31.85555556, 15.6       , 21.10555556, 15.18      ,\n",
       "       20.21176471, 19.48181818, 15.6       , 22.13333333, 19.26285714])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:01:40.341885Z",
     "start_time": "2019-05-24T08:01:40.336870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.6, 32.4, 13.6, 22.8, 16.1, 20. , 17.8, 14. , 19.6, 16.8])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:01:40.579310Z",
     "start_time": "2019-05-24T08:01:40.576781Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:01:41.338136Z",
     "start_time": "2019-05-24T08:01:41.333514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.18004748583447"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions,target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:01:42.109950Z",
     "start_time": "2019-05-24T08:01:42.105722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.3"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(predictions-target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T08:01:45.141283Z",
     "start_time": "2019-05-24T08:01:45.137481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.199999999999999"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(predictions-target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have some bad errors but our predictions are sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
